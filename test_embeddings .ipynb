{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nulber of abstracts: 27770\n"
     ]
    }
   ],
   "source": [
    "#training word embeddings on the abstract of the node information \n",
    "abstracts = [element[5] for element in node_info ]\n",
    "print(\"total nulber of abstracts: %d\" %len(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstracts_w = [element.lower().split() for element in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 1min 3s, sys: 64 ms, total: 1min 3s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(abstracts_w, workers=num_workers, size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        if counter%1000. ==0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "       \n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/asus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#create word list for each abstract without stop words\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "abstracts_stp =  [[word for word in element.split(\" \") if word.lower() not in stpwds] for element in abstracts ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 27770\n",
      "Review 1000 of 27770\n",
      "Review 2000 of 27770\n",
      "Review 3000 of 27770\n",
      "Review 4000 of 27770\n",
      "Review 5000 of 27770\n",
      "Review 6000 of 27770\n",
      "Review 7000 of 27770\n",
      "Review 8000 of 27770\n",
      "Review 9000 of 27770\n",
      "Review 10000 of 27770\n",
      "Review 11000 of 27770\n",
      "Review 12000 of 27770\n",
      "Review 13000 of 27770\n",
      "Review 14000 of 27770\n",
      "Review 15000 of 27770\n",
      "Review 16000 of 27770\n",
      "Review 17000 of 27770\n",
      "Review 18000 of 27770\n",
      "Review 19000 of 27770\n",
      "Review 20000 of 27770\n",
      "Review 21000 of 27770\n",
      "Review 22000 of 27770\n",
      "Review 23000 of 27770\n",
      "Review 24000 of 27770\n",
      "Review 25000 of 27770\n",
      "Review 26000 of 27770\n",
      "Review 27000 of 27770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "DataVecs = getAvgFeatureVecs( abstracts_stp, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27770, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/asus/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "s_1 = DataVecs[0]\n",
    "s_2 = DataVecs[1]\n",
    "print round(cosine(s_1,s_2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "def isselfcite(source_auth, target_auth): \n",
    "    selfcite = 0\n",
    "    for sauth in source_auth: \n",
    "        if sauth in target_auth: \n",
    "            selfcite = 1\n",
    "            break \n",
    "    return selfcite\n",
    "\n",
    "def issamejournal(source_journal, target_journal): \n",
    "    \n",
    "    if source_journal == target_journal: \n",
    "        same_journal = 1\n",
    "    else:\n",
    "        same_journal = 0\n",
    "    return same_journal\n",
    "        \n",
    "        \n",
    "def cosine_similarity(s_1, s_2): \n",
    "    #remove stopwords \n",
    "    s_1 = np.reshape(s_1,(1,-1)  )\n",
    "    s_2 = np.reshape(s_2,(1,-1)  )\n",
    "    return round(cosine(s_1,s_2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in this baseline we will use three basic features:\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "#is self citation\n",
    "self_cite = []\n",
    "\n",
    "#is published in same journal \n",
    "same_journal = []\n",
    "\n",
    "#cosine  similarity \n",
    "cosine_sim = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "77001 training examples processsed\n",
      "78001 training examples processsed\n",
      "79001 training examples processsed\n",
      "80001 training examples processsed\n",
      "81001 training examples processsed\n",
      "82001 training examples processsed\n",
      "83001 training examples processsed\n",
      "84001 training examples processsed\n",
      "85001 training examples processsed\n",
      "86001 training examples processsed\n",
      "87001 training examples processsed\n",
      "88001 training examples processsed\n",
      "89001 training examples processsed\n",
      "90001 training examples processsed\n",
      "91001 training examples processsed\n",
      "92001 training examples processsed\n",
      "93001 training examples processsed\n",
      "94001 training examples processsed\n",
      "95001 training examples processsed\n",
      "96001 training examples processsed\n",
      "97001 training examples processsed\n",
      "98001 training examples processsed\n",
      "99001 training examples processsed\n",
      "100001 training examples processsed\n",
      "101001 training examples processsed\n",
      "102001 training examples processsed\n",
      "103001 training examples processsed\n",
      "104001 training examples processsed\n",
      "105001 training examples processsed\n",
      "106001 training examples processsed\n",
      "107001 training examples processsed\n",
      "108001 training examples processsed\n",
      "109001 training examples processsed\n",
      "110001 training examples processsed\n",
      "111001 training examples processsed\n",
      "112001 training examples processsed\n",
      "113001 training examples processsed\n",
      "114001 training examples processsed\n",
      "115001 training examples processsed\n",
      "116001 training examples processsed\n",
      "117001 training examples processsed\n",
      "118001 training examples processsed\n",
      "119001 training examples processsed\n",
      "120001 training examples processsed\n",
      "121001 training examples processsed\n",
      "122001 training examples processsed\n",
      "123001 training examples processsed\n",
      "124001 training examples processsed\n",
      "125001 training examples processsed\n",
      "126001 training examples processsed\n",
      "127001 training examples processsed\n",
      "128001 training examples processsed\n",
      "129001 training examples processsed\n",
      "130001 training examples processsed\n",
      "131001 training examples processsed\n",
      "132001 training examples processsed\n",
      "133001 training examples processsed\n",
      "134001 training examples processsed\n",
      "135001 training examples processsed\n",
      "136001 training examples processsed\n",
      "137001 training examples processsed\n",
      "138001 training examples processsed\n",
      "139001 training examples processsed\n",
      "140001 training examples processsed\n",
      "141001 training examples processsed\n",
      "142001 training examples processsed\n",
      "143001 training examples processsed\n",
      "144001 training examples processsed\n",
      "145001 training examples processsed\n",
      "146001 training examples processsed\n",
      "147001 training examples processsed\n",
      "148001 training examples processsed\n",
      "149001 training examples processsed\n",
      "150001 training examples processsed\n",
      "151001 training examples processsed\n",
      "152001 training examples processsed\n",
      "153001 training examples processsed\n",
      "154001 training examples processsed\n",
      "155001 training examples processsed\n",
      "156001 training examples processsed\n",
      "157001 training examples processsed\n",
      "158001 training examples processsed\n",
      "159001 training examples processsed\n",
      "160001 training examples processsed\n",
      "161001 training examples processsed\n",
      "162001 training examples processsed\n",
      "163001 training examples processsed\n",
      "164001 training examples processsed\n",
      "165001 training examples processsed\n",
      "166001 training examples processsed\n",
      "167001 training examples processsed\n",
      "168001 training examples processsed\n",
      "169001 training examples processsed\n",
      "170001 training examples processsed\n",
      "171001 training examples processsed\n",
      "172001 training examples processsed\n",
      "173001 training examples processsed\n",
      "174001 training examples processsed\n",
      "175001 training examples processsed\n",
      "176001 training examples processsed\n",
      "177001 training examples processsed\n",
      "178001 training examples processsed\n",
      "179001 training examples processsed\n",
      "180001 training examples processsed\n",
      "181001 training examples processsed\n",
      "182001 training examples processsed\n",
      "183001 training examples processsed\n",
      "184001 training examples processsed\n",
      "185001 training examples processsed\n",
      "186001 training examples processsed\n",
      "187001 training examples processsed\n",
      "188001 training examples processsed\n",
      "189001 training examples processsed\n",
      "190001 training examples processsed\n",
      "191001 training examples processsed\n",
      "192001 training examples processsed\n",
      "193001 training examples processsed\n",
      "194001 training examples processsed\n",
      "195001 training examples processsed\n",
      "196001 training examples processsed\n",
      "197001 training examples processsed\n",
      "198001 training examples processsed\n",
      "199001 training examples processsed\n",
      "200001 training examples processsed\n",
      "201001 training examples processsed\n",
      "202001 training examples processsed\n",
      "203001 training examples processsed\n",
      "204001 training examples processsed\n",
      "205001 training examples processsed\n",
      "206001 training examples processsed\n",
      "207001 training examples processsed\n",
      "208001 training examples processsed\n",
      "209001 training examples processsed\n",
      "210001 training examples processsed\n",
      "211001 training examples processsed\n",
      "212001 training examples processsed\n",
      "213001 training examples processsed\n",
      "214001 training examples processsed\n",
      "215001 training examples processsed\n",
      "216001 training examples processsed\n",
      "217001 training examples processsed\n",
      "218001 training examples processsed\n",
      "219001 training examples processsed\n",
      "220001 training examples processsed\n",
      "221001 training examples processsed\n",
      "222001 training examples processsed\n",
      "223001 training examples processsed\n",
      "224001 training examples processsed\n",
      "225001 training examples processsed\n",
      "226001 training examples processsed\n",
      "227001 training examples processsed\n",
      "228001 training examples processsed\n",
      "229001 training examples processsed\n",
      "230001 training examples processsed\n",
      "231001 training examples processsed\n",
      "232001 training examples processsed\n",
      "233001 training examples processsed\n",
      "234001 training examples processsed\n",
      "235001 training examples processsed\n",
      "236001 training examples processsed\n",
      "237001 training examples processsed\n",
      "238001 training examples processsed\n",
      "239001 training examples processsed\n",
      "240001 training examples processsed\n",
      "241001 training examples processsed\n",
      "242001 training examples processsed\n",
      "243001 training examples processsed\n",
      "244001 training examples processsed\n",
      "245001 training examples processsed\n",
      "246001 training examples processsed\n",
      "247001 training examples processsed\n",
      "248001 training examples processsed\n",
      "249001 training examples processsed\n",
      "250001 training examples processsed\n",
      "251001 training examples processsed\n",
      "252001 training examples processsed\n",
      "253001 training examples processsed\n",
      "254001 training examples processsed\n",
      "255001 training examples processsed\n",
      "256001 training examples processsed\n",
      "257001 training examples processsed\n",
      "258001 training examples processsed\n",
      "259001 training examples processsed\n",
      "260001 training examples processsed\n",
      "261001 training examples processsed\n",
      "262001 training examples processsed\n",
      "263001 training examples processsed\n",
      "264001 training examples processsed\n",
      "265001 training examples processsed\n",
      "266001 training examples processsed\n",
      "267001 training examples processsed\n",
      "268001 training examples processsed\n",
      "269001 training examples processsed\n",
      "270001 training examples processsed\n",
      "271001 training examples processsed\n",
      "272001 training examples processsed\n",
      "273001 training examples processsed\n",
      "274001 training examples processsed\n",
      "275001 training examples processsed\n",
      "276001 training examples processsed\n",
      "277001 training examples processsed\n",
      "278001 training examples processsed\n",
      "279001 training examples processsed\n",
      "280001 training examples processsed\n",
      "281001 training examples processsed\n",
      "282001 training examples processsed\n",
      "283001 training examples processsed\n",
      "284001 training examples processsed\n",
      "285001 training examples processsed\n",
      "286001 training examples processsed\n",
      "287001 training examples processsed\n",
      "288001 training examples processsed\n",
      "289001 training examples processsed\n",
      "290001 training examples processsed\n",
      "291001 training examples processsed\n",
      "292001 training examples processsed\n",
      "293001 training examples processsed\n",
      "294001 training examples processsed\n",
      "295001 training examples processsed\n",
      "296001 training examples processsed\n",
      "297001 training examples processsed\n",
      "298001 training examples processsed\n",
      "299001 training examples processsed\n",
      "300001 training examples processsed\n",
      "301001 training examples processsed\n",
      "302001 training examples processsed\n",
      "303001 training examples processsed\n",
      "304001 training examples processsed\n",
      "305001 training examples processsed\n",
      "306001 training examples processsed\n",
      "307001 training examples processsed\n",
      "308001 training examples processsed\n",
      "309001 training examples processsed\n",
      "310001 training examples processsed\n",
      "311001 training examples processsed\n",
      "312001 training examples processsed\n",
      "313001 training examples processsed\n",
      "314001 training examples processsed\n",
      "315001 training examples processsed\n",
      "316001 training examples processsed\n",
      "317001 training examples processsed\n",
      "318001 training examples processsed\n",
      "319001 training examples processsed\n",
      "320001 training examples processsed\n",
      "321001 training examples processsed\n",
      "322001 training examples processsed\n",
      "323001 training examples processsed\n",
      "324001 training examples processsed\n",
      "325001 training examples processsed\n",
      "326001 training examples processsed\n",
      "327001 training examples processsed\n",
      "328001 training examples processsed\n",
      "329001 training examples processsed\n",
      "330001 training examples processsed\n",
      "331001 training examples processsed\n",
      "332001 training examples processsed\n",
      "333001 training examples processsed\n",
      "334001 training examples processsed\n",
      "335001 training examples processsed\n",
      "336001 training examples processsed\n",
      "337001 training examples processsed\n",
      "338001 training examples processsed\n",
      "339001 training examples processsed\n",
      "340001 training examples processsed\n",
      "341001 training examples processsed\n",
      "342001 training examples processsed\n",
      "343001 training examples processsed\n",
      "344001 training examples processsed\n",
      "345001 training examples processsed\n",
      "346001 training examples processsed\n",
      "347001 training examples processsed\n",
      "348001 training examples processsed\n",
      "349001 training examples processsed\n",
      "350001 training examples processsed\n",
      "351001 training examples processsed\n",
      "352001 training examples processsed\n",
      "353001 training examples processsed\n",
      "354001 training examples processsed\n",
      "355001 training examples processsed\n",
      "356001 training examples processsed\n",
      "357001 training examples processsed\n",
      "358001 training examples processsed\n",
      "359001 training examples processsed\n",
      "360001 training examples processsed\n",
      "361001 training examples processsed\n",
      "362001 training examples processsed\n",
      "363001 training examples processsed\n",
      "364001 training examples processsed\n",
      "365001 training examples processsed\n",
      "366001 training examples processsed\n",
      "367001 training examples processsed\n",
      "368001 training examples processsed\n",
      "369001 training examples processsed\n",
      "370001 training examples processsed\n",
      "371001 training examples processsed\n",
      "372001 training examples processsed\n",
      "373001 training examples processsed\n",
      "374001 training examples processsed\n",
      "375001 training examples processsed\n",
      "376001 training examples processsed\n",
      "377001 training examples processsed\n",
      "378001 training examples processsed\n",
      "379001 training examples processsed\n",
      "380001 training examples processsed\n",
      "381001 training examples processsed\n",
      "382001 training examples processsed\n",
      "383001 training examples processsed\n",
      "384001 training examples processsed\n",
      "385001 training examples processsed\n",
      "386001 training examples processsed\n",
      "387001 training examples processsed\n",
      "388001 training examples processsed\n",
      "389001 training examples processsed\n",
      "390001 training examples processsed\n",
      "391001 training examples processsed\n",
      "392001 training examples processsed\n",
      "393001 training examples processsed\n",
      "394001 training examples processsed\n",
      "395001 training examples processsed\n",
      "396001 training examples processsed\n",
      "397001 training examples processsed\n",
      "398001 training examples processsed\n",
      "399001 training examples processsed\n",
      "400001 training examples processsed\n",
      "401001 training examples processsed\n",
      "402001 training examples processsed\n",
      "403001 training examples processsed\n",
      "404001 training examples processsed\n",
      "405001 training examples processsed\n",
      "406001 training examples processsed\n",
      "407001 training examples processsed\n",
      "408001 training examples processsed\n",
      "409001 training examples processsed\n",
      "410001 training examples processsed\n",
      "411001 training examples processsed\n",
      "412001 training examples processsed\n",
      "413001 training examples processsed\n",
      "414001 training examples processsed\n",
      "415001 training examples processsed\n",
      "416001 training examples processsed\n",
      "417001 training examples processsed\n",
      "418001 training examples processsed\n",
      "419001 training examples processsed\n",
      "420001 training examples processsed\n",
      "421001 training examples processsed\n",
      "422001 training examples processsed\n",
      "423001 training examples processsed\n",
      "424001 training examples processsed\n",
      "425001 training examples processsed\n",
      "426001 training examples processsed\n",
      "427001 training examples processsed\n",
      "428001 training examples processsed\n",
      "429001 training examples processsed\n",
      "430001 training examples processsed\n",
      "431001 training examples processsed\n",
      "432001 training examples processsed\n",
      "433001 training examples processsed\n",
      "434001 training examples processsed\n",
      "435001 training examples processsed\n",
      "436001 training examples processsed\n",
      "437001 training examples processsed\n",
      "438001 training examples processsed\n",
      "439001 training examples processsed\n",
      "440001 training examples processsed\n",
      "441001 training examples processsed\n",
      "442001 training examples processsed\n",
      "443001 training examples processsed\n",
      "444001 training examples processsed\n",
      "445001 training examples processsed\n",
      "446001 training examples processsed\n",
      "447001 training examples processsed\n",
      "448001 training examples processsed\n",
      "449001 training examples processsed\n",
      "450001 training examples processsed\n",
      "451001 training examples processsed\n",
      "452001 training examples processsed\n",
      "453001 training examples processsed\n",
      "454001 training examples processsed\n",
      "455001 training examples processsed\n",
      "456001 training examples processsed\n",
      "457001 training examples processsed\n",
      "458001 training examples processsed\n",
      "459001 training examples processsed\n",
      "460001 training examples processsed\n",
      "461001 training examples processsed\n",
      "462001 training examples processsed\n",
      "463001 training examples processsed\n",
      "464001 training examples processsed\n",
      "465001 training examples processsed\n",
      "466001 training examples processsed\n",
      "467001 training examples processsed\n",
      "468001 training examples processsed\n",
      "469001 training examples processsed\n",
      "470001 training examples processsed\n",
      "471001 training examples processsed\n",
      "472001 training examples processsed\n",
      "473001 training examples processsed\n",
      "474001 training examples processsed\n",
      "475001 training examples processsed\n",
      "476001 training examples processsed\n",
      "477001 training examples processsed\n",
      "478001 training examples processsed\n",
      "479001 training examples processsed\n",
      "480001 training examples processsed\n",
      "481001 training examples processsed\n",
      "482001 training examples processsed\n",
      "483001 training examples processsed\n",
      "484001 training examples processsed\n",
      "485001 training examples processsed\n",
      "486001 training examples processsed\n",
      "487001 training examples processsed\n",
      "488001 training examples processsed\n",
      "489001 training examples processsed\n",
      "490001 training examples processsed\n",
      "491001 training examples processsed\n",
      "492001 training examples processsed\n",
      "493001 training examples processsed\n",
      "494001 training examples processsed\n",
      "495001 training examples processsed\n",
      "496001 training examples processsed\n",
      "497001 training examples processsed\n",
      "498001 training examples processsed\n",
      "499001 training examples processsed\n",
      "500001 training examples processsed\n",
      "501001 training examples processsed\n",
      "502001 training examples processsed\n",
      "503001 training examples processsed\n",
      "504001 training examples processsed\n",
      "505001 training examples processsed\n",
      "506001 training examples processsed\n",
      "507001 training examples processsed\n",
      "508001 training examples processsed\n",
      "509001 training examples processsed\n",
      "510001 training examples processsed\n",
      "511001 training examples processsed\n",
      "512001 training examples processsed\n",
      "513001 training examples processsed\n",
      "514001 training examples processsed\n",
      "515001 training examples processsed\n",
      "516001 training examples processsed\n",
      "517001 training examples processsed\n",
      "518001 training examples processsed\n",
      "519001 training examples processsed\n",
      "520001 training examples processsed\n",
      "521001 training examples processsed\n",
      "522001 training examples processsed\n",
      "523001 training examples processsed\n",
      "524001 training examples processsed\n",
      "525001 training examples processsed\n",
      "526001 training examples processsed\n",
      "527001 training examples processsed\n",
      "528001 training examples processsed\n",
      "529001 training examples processsed\n",
      "530001 training examples processsed\n",
      "531001 training examples processsed\n",
      "532001 training examples processsed\n",
      "533001 training examples processsed\n",
      "534001 training examples processsed\n",
      "535001 training examples processsed\n",
      "536001 training examples processsed\n",
      "537001 training examples processsed\n",
      "538001 training examples processsed\n",
      "539001 training examples processsed\n",
      "540001 training examples processsed\n",
      "541001 training examples processsed\n",
      "542001 training examples processsed\n",
      "543001 training examples processsed\n",
      "544001 training examples processsed\n",
      "545001 training examples processsed\n",
      "546001 training examples processsed\n",
      "547001 training examples processsed\n",
      "548001 training examples processsed\n",
      "549001 training examples processsed\n",
      "550001 training examples processsed\n",
      "551001 training examples processsed\n",
      "552001 training examples processsed\n",
      "553001 training examples processsed\n",
      "554001 training examples processsed\n",
      "555001 training examples processsed\n",
      "556001 training examples processsed\n",
      "557001 training examples processsed\n",
      "558001 training examples processsed\n",
      "559001 training examples processsed\n",
      "560001 training examples processsed\n",
      "561001 training examples processsed\n",
      "562001 training examples processsed\n",
      "563001 training examples processsed\n",
      "564001 training examples processsed\n",
      "565001 training examples processsed\n",
      "566001 training examples processsed\n",
      "567001 training examples processsed\n",
      "568001 training examples processsed\n",
      "569001 training examples processsed\n",
      "570001 training examples processsed\n",
      "571001 training examples processsed\n",
      "572001 training examples processsed\n",
      "573001 training examples processsed\n",
      "574001 training examples processsed\n",
      "575001 training examples processsed\n",
      "576001 training examples processsed\n",
      "577001 training examples processsed\n",
      "578001 training examples processsed\n",
      "579001 training examples processsed\n",
      "580001 training examples processsed\n",
      "581001 training examples processsed\n",
      "582001 training examples processsed\n",
      "583001 training examples processsed\n",
      "584001 training examples processsed\n",
      "585001 training examples processsed\n",
      "586001 training examples processsed\n",
      "587001 training examples processsed\n",
      "588001 training examples processsed\n",
      "589001 training examples processsed\n",
      "590001 training examples processsed\n",
      "591001 training examples processsed\n",
      "592001 training examples processsed\n",
      "593001 training examples processsed\n",
      "594001 training examples processsed\n",
      "595001 training examples processsed\n",
      "596001 training examples processsed\n",
      "597001 training examples processsed\n",
      "598001 training examples processsed\n",
      "599001 training examples processsed\n",
      "600001 training examples processsed\n",
      "601001 training examples processsed\n",
      "602001 training examples processsed\n",
      "603001 training examples processsed\n",
      "604001 training examples processsed\n",
      "605001 training examples processsed\n",
      "606001 training examples processsed\n",
      "607001 training examples processsed\n",
      "608001 training examples processsed\n",
      "609001 training examples processsed\n",
      "610001 training examples processsed\n",
      "611001 training examples processsed\n",
      "612001 training examples processsed\n",
      "613001 training examples processsed\n",
      "614001 training examples processsed\n",
      "615001 training examples processsed\n",
      "CPU times: user 1h 27min 16s, sys: 3.7 s, total: 1h 27min 19s\n",
      "Wall time: 1h 27min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "counter = 0\n",
    "for i in xrange(len(training_set)):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    source_journal = source_info[4].lower()\n",
    "    target_journal = target_info[4].lower()\n",
    "    \n",
    "    source_abstract = DataVecs[index_source]\n",
    "    target_abstract = DataVecs[index_target]\n",
    "    \n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    self_cite.append(isselfcite(source_auth,target_auth))\n",
    "    same_journal.append(issamejournal(source_journal, target_journal))\n",
    "    cosine_sim.append(cosine_similarity(source_abstract, target_abstract))\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transforming test features \n",
    "# number of overlapping words in title\n",
    "overlap_title_test = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff_test = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth_test = []\n",
    "\n",
    "#is self citation\n",
    "self_cite_test = []\n",
    "\n",
    "#is published in same journal \n",
    "same_journal_test = []\n",
    "\n",
    "#cosine  similarity \n",
    "cosine_sim_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test examples processsed\n",
      "1001 test examples processsed\n",
      "2001 test examples processsed\n",
      "3001 test examples processsed\n",
      "4001 test examples processsed\n",
      "5001 test examples processsed\n",
      "6001 test examples processsed\n",
      "7001 test examples processsed\n",
      "8001 test examples processsed\n",
      "9001 test examples processsed\n",
      "10001 test examples processsed\n",
      "11001 test examples processsed\n",
      "12001 test examples processsed\n",
      "13001 test examples processsed\n",
      "14001 test examples processsed\n",
      "15001 test examples processsed\n",
      "16001 test examples processsed\n",
      "17001 test examples processsed\n",
      "18001 test examples processsed\n",
      "19001 test examples processsed\n",
      "20001 test examples processsed\n",
      "21001 test examples processsed\n",
      "22001 test examples processsed\n",
      "23001 test examples processsed\n",
      "24001 test examples processsed\n",
      "25001 test examples processsed\n",
      "26001 test examples processsed\n",
      "27001 test examples processsed\n",
      "28001 test examples processsed\n",
      "29001 test examples processsed\n",
      "30001 test examples processsed\n",
      "31001 test examples processsed\n",
      "32001 test examples processsed\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    source_journal = source_info[4].lower()\n",
    "    target_journal = target_info[4].lower()\n",
    "    \n",
    "    source_abstract = DataVecs[index_source]\n",
    "    target_abstract = DataVecs[index_target]\n",
    "    \n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    self_cite_test.append(isselfcite(source_auth,target_auth))\n",
    "    same_journal_test.append(issamejournal(source_journal, target_journal))\n",
    "    cosine_sim_test.append(cosine_similarity(source_abstract, target_abstract))\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print counter, \"test examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_features = np.array([overlap_title_test, temp_diff_test, comm_auth_test,cosine_sim_test,same_journal_test, self_cite_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features = np.array([overlap_title, temp_diff, comm_auth,cosine_sim,same_journal, self_cite]).T\n",
    "training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615512, 6)\n",
      "(32648, 6)\n"
     ]
    }
   ],
   "source": [
    "print training_features.shape \n",
    "print testing_features.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('training_features_6.csv', training_features, delimiter=\",\")\n",
    "np.savetxt('testing_features_6.csv', testing_features, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
